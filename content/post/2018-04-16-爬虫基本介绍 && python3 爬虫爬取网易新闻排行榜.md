---
title: "爬虫基本介绍 && python3 爬虫爬取网易新闻排行榜"
date: "2018-04-16"
categories: 
  - "coding"
tags: 
  - "python"
  - "爬虫"
url: "/archives/293"
---

* * *

## 1\. 什么是爬虫？

爬虫是请求⽹网站并提取数据的⾃自动化程序

## 2\. 爬虫的基本流程

1. **发起请求** 通过HTTP库向目标站点发起请求，即发送一个Request，请求可以包含额外的headers等信息，等待服务器器响应。
2. **解析内容** 如果服务器能正常响应，会得到一个Response，Response的内容便是所要获取的页面内容，类型可能有HTML，Json字符串，二进制数据（如图片视频）等类型。
3. **获取响应内容** 得到的内容可能是HTML，可以用正则表达式、网页解析库进行解析。可能是Json，可以直接转为Json对象解析，可能是二进制数据，可以做保存或者进一步的处理。
4. **保存数据** 保存形式多样，可以存为文本，也可以保存至数据库，或者保存特定格式的文件。

## 3\. 什么是Request和Response?

![Request和Response](https://image.i-ll.cc/18-4-16/68442293.jpg)

- 浏览器就发送消息给该网址所在的服务器，这个过程叫做HTTP Request。
- 服务器收到浏览器发送的消息后，能够根据浏览器发送消息的内容，做相应处理，然后把消息回传给浏览器。这个过程叫做HTTP Response。
- 浏览器收到服务器的Response信息后，会对信息进行相应处理，然后展示。

### 3.1 Request中包含什么？

- **请求方式**：主要有GET、POST两种类型，另外还有HEAD、PUT、DELETE、OPTIONS等。
- **请求头**：包含请求时的头部信息，如User-Agent、Host、Cookies等信息。
- **请求URL**：URL全称统一资源定位符，如一个网页文档、一张图片、一个视频等都可以用URL唯一来确定。
- **请求体**：请求时额外携带的数据如表单提交时的表单数据

### 3.2 Response中包含什么？

- **响应状态**：有多种响应状态，如200代表成功、301跳转、404找不到页面、502服务器错误
- **响应头**：如内容类型、内容长度、服务器信息、设置Cookie等等。
- **响应体**：最主要的部分，包含了请求资源的内容，如网页HTML、图片二进制数据等。

## 4\. 爬虫可以抓取怎样的数据？

- 网页文本：如HTML文档、Json格式文本等。
- 图片：获取到的是二进制文件，保存为图片格式。
- 视频：同为二进制文件，保存为视频格式即可。
- 其它：只要是能请求到的，都能获取。

## 5\. 怎样来解析？

- 直接处理
- Json解析
- 正则表达式
- BeautifulSoup
- PyQuery
- Xpath

## 6\. 怎样保存数据？

- **文本**：纯文本、Json、Xml等。
- **关系型数据库**：如MySQL、Oracle、SQL Server等具有结构化表结构形式存储。
- **非关系型数据库**：如MongoDB、Redis等Key-Value形式存储。
- **二进制文件**：如图片、视频、音频等直接保存成特定格式即可。

* * *

# 爬虫爬取网易新闻排行榜。

* * *

## 1\. 确定目标

经过讨论，分析，对比各个新闻门户网站的排行榜，最终选取了内容正规、不良信息少、广告少的网易新闻排行榜。每个整点爬取一次，选取点击率最高的前20条热门新闻。即红框所选内容。 ![网易新闻排行榜](https://image.i-ll.cc/18-4-15/89071865.jpg)

## 2\. 分析目标网页

目标网页：http://news.163.com/special/0001386F/rank\_news.html 通过分析网页源代码得知，这个网页排行榜并不是通过JavaScript动态加载生成。所以网页爬取后可以直接处理，简单利用BeautifulSoup + requests库即可实现。

## 3\. 实现过程

### 3.1 请求目标网页并存储网页源文件

```
# 导入相应的包
from bs4 import BeautifulSoup
import requests
import re
import datetime
from connect_mysql import *
# 获取当前时间并指定格式为2018041018
time = datetime.datetime.now().strftime("%Y%m%d%H")
url = r'http://news.163.com/special/0001386F/rank_news.html'
# 模拟真实浏览器进行访问
headers = {'User-Agent':
               'Mozilla/5.0 (Windows NT 10.0; WOW64) '
               'AppleWebKit/537.36 (KHTML, like Gecko) '
               'Chrome/55.0.2883.87 Safari/537.36'}
response = requests.get(url, headers=headers)
page_html = response.text
```

###3.2 将获取到的内容转换成BeautifulSoup格式，并将html.parser作为解析器

```
soup = BeautifulSoup(page_html, 'html.parser')
```

###3.3 对soup进行分析处理 首先通过分析网站结构得知，我们需要的数据在class = tabContents active的div下的所有a标签中，而这个div又嵌套在class = area-half left的div中。所以我们写如下代码，从网页源代码中找到所有符合要求的标题，限制20条。

```
titles = soup.find('div', 'area-half left').find('div', 'tabContents active').find_all('a', limit=20)
```

仅需一行代码，我们就已经获取到了我们需要的部分数据。接下来继续获取新的对我们有用的数据，并存放到数据库。

![通过开发者调试工具分析网页源代码](https://image.i-ll.cc/18-4-16/92160852.jpg)

### 3.4 对3.3的结果进一步处理

由于网页标题显示不全，所以上一步爬取的标题，部分不完整。所以我们需要进一步爬取，步骤和上述几个操作类似，爬取新闻链接，新闻内容，新闻完整标题。这三个内容，并调用自己写的方法，将数据存放到数据库。并用re库对内容进行简单处理(去除\\n\\t\\r )，具体实现如下。

```
for title in titles:
    '''
    news_url:新闻链接
    news_html:新闻页网页源代码 
    '''
    news_url = (str(title.get('href')))
    news_response = requests.get(news_url, headers=headers)
    news_html = news_response.text
    # 将获取到的内容转换成BeautifulSoup格式，并将html.parser作为解析器
    news_soup = BeautifulSoup(news_html, 'html.parser')
    # 从网页源代码中找到属于post_text类的div，并将所有p标签内容存入列表news_contents
    if news_soup.find('div', 'post_text') is None:  # 如果网页丢失,跳出本次循环
        continue
    news_title = news_soup.find('h1')
    contents = news_soup.find('div', 'post_text').find_all('p')
    news_contents = []
    for content in contents:
        if content.string is not None:
            #去掉特殊字符
            news_contents.append(re.sub('[\r\n\t ]', '', str(content.string)))
    #字符串拼接
    news_contents = ''.join(news_contents)
    # 将爬取到的数据存入数据库
    insert_wangyinews_into_mysql(wangyi_news, str(news_title.string), news_url, news_contents, time)
```

这样，一个简单的爬虫就完成了，不要小看这几十行代码，它们在本项目中发挥了巨大的作用。

**本文所用开发环境：** **anaconda 5.1** **pycharm**

本文第一部分是崔庆才老师的python3网络爬虫视频教程的笔记： 课程链接：https://cuiqingcai.com/4320.html 崔老师的课程对我有很大的帮助，再次感谢。
